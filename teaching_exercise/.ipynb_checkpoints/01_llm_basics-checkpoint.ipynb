{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ffe346-ec28-4c7e-9acb-5d6df02070e7",
   "metadata": {},
   "source": [
    "# Session 2 – Notebook 2: LLM Basics\n",
    "\n",
    "### **Objectives:**\n",
    "By the end of this notebook, you will:\n",
    "- Use a pre-trained Large Language Model (LLM) from Hugging Face\r\n",
    "- Understand how LLMs generate text from prompts\r\n",
    "- Observe how prompt phrasing affects responscy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a04f7-9c85-4fed-ae86-82daf8196f01",
   "metadata": {},
   "source": [
    "### What is `transformers`?\n",
    "`transformers` (by Hugging Face) is a Python library that provides:\n",
    "- **models** (pretrained neural networks)\n",
    "- **tokenizers** (text → tokens)\n",
    "- **pipelines** (easy wrapper that combines everything)\n",
    "\n",
    "Today we use `pipeline` so we don’t manually handle tokenizers/models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63d4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM CHATBOT (using Hugging Face pipeline)\n",
    "import warnings\n",
    "\n",
    "# Optional: hide noisy FutureWarnings in teaching demos\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import the Hugging Face pipeline utility\n",
    "# `pipeline(...)` is a high-level wrapper that bundles:\n",
    "# 1) tokenizer  2) model  3) text generation output formatting\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a \"text2text-generation\" pipeline:\n",
    "# - Input : text prompt (string)\n",
    "# - Output: generated text\n",
    "#\n",
    "# model=\"google/flan-t5-small\" is a lightweight instruction-following model (CPU friendly)\n",
    "gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd89da-b033-4682-9580-8ea84099eed7",
   "metadata": {},
   "source": [
    "### Core syntax (you will use this everywhere)\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "response = gen(prompt, max_new_tokens=30)\n",
    "print(response[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "- **prompt** is the input text (string)\n",
    "- **max_new_tokens** limits how long the output can be\n",
    "- **gen(...)** is our Hugging Face pipeline (a callable object)\n",
    "- **response** is a list (can contain multiple results), **response[0]** is the first result (a dictionary)\n",
    "- **[\"generated_text\"]** is the model’s output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77138780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Respond politely to the greeting: Hey\n",
      "LLM Bot: Hey, how are you?\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Greeting\n",
    "\n",
    "prompt = \"Respond politely to the greeting: Hey\"\n",
    "print(\"Prompt:\", prompt)\n",
    "\n",
    "# Ask the model to generate a response (limit to 30 new tokens for short answers)\n",
    "response = gen(prompt, max_new_tokens=30)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e326238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Answer in one sentence: Explain what is AI and where is it used?\n",
      "LLM Bot: Artificial intelligence\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Open-ended question\n",
    "\n",
    "prompt = \"Answer in one sentence: Explain what is AI and where is it used?\"\n",
    "print(\"User:\", prompt)\n",
    "\n",
    "response = gen(prompt, max_new_tokens=30)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f72844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Summarize: A cool cat went to the library to play lots of games and read lots of books at the arcade.\n",
      "LLM Bot: A cat went to the library to play games and read books.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: A creative question (not possible in rule-based bot)\n",
    "\n",
    "prompt = (\n",
    "    \"Summarize: A cool cat went to the library to play lots of games and read lots of books at the arcade.\"\n",
    ")\n",
    "\n",
    "print(\"User:\", prompt)\n",
    "\n",
    "response = gen(prompt, max_new_tokens=60)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ac7fb",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    "- Did the LLM understand \"Hey\" even though we didn’t code a rule?  \n",
    "- What are the risks of using an LLM (e.g., making things up)?\n",
    "- Did the same prompt give differenout responses? Did changing the prompt change the output style?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be088fbd-9310-4862-a4ed-fed5cea3ba17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
