{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3eb8f2-240e-4d75-9d72-d9c6775a008f",
   "metadata": {},
   "source": [
    "# Session 3 ‚Äî Notebook 02: LLM Parameters\n",
    "\n",
    "### Objective: \n",
    "Learn the **syntax** for controlling LLM outputs using generation parameters.\n",
    "\n",
    "We will experiment with:\n",
    "- `max_new_tokens` ‚Üí response length\n",
    "- `do_sample` ‚Üí deterministic vs varied outputs\n",
    "- `temperature` ‚Üí randomness (works with sampling)\n",
    "- `top_p` ‚Üí controls how broad the word choice pool is (works with sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903c6438-8232-40e3-99b3-3551e362af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # optional for cleaner class output\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load an instruction-following model (CPU friendly)\n",
    "# google/flan-t5-base ‚Üí higher quality than \"small\" (but slower/heavier on CPU)\n",
    "gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8843e8-de11-4bb8-a743-d69ceadeb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_prompt(task: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful assistant for students. \"\n",
    "        \"Keep responses polite, non-explicit, and suitable for a classroom.\\n\"\n",
    "        \"If the task is unclear, ask a short clarifying question.\\n\"\n",
    "        f\"Task: {task}\"\n",
    "    )\n",
    "\n",
    "# We'll reuse one prompt across experiments to compare outputs\n",
    "prompt = safe_prompt(\"Write a short story about a dog who becomes a hero.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781cc0c-d7d2-4fe0-963f-ed01d706c830",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40380cbf-20e7-4ba3-af8a-eebdb5c6e6f7",
   "metadata": {},
   "source": [
    "### 1) `max_new_tokens` -> response length\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "response = gen(prompt, max_new_tokens=30)\n",
    "```\n",
    "\n",
    "- max_new_tokens limits how many new tokens the model can generate.\n",
    "- Smaller value ‚Üí shorter response, Larger value ‚Üí longer response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e05ca4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens = 20\n",
      "The dog is a dog. He is a dog. He is a hero.\n"
     ]
    }
   ],
   "source": [
    "# MAX NEW TOKENS -> controls how long the response can be\n",
    "# Smaller values = short answers\n",
    "\n",
    "print(\"Max tokens = 20\")\n",
    "print(gen(prompt, max_new_tokens=20)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3573a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max tokens = 80\n",
      "The dog is a dog. He is a dog. He is a hero. He is a dog. He is a hero.\n"
     ]
    }
   ],
   "source": [
    "# MAX NEW TOKENS -> controls how long the response can be\n",
    "# Larger values = detailed answers\n",
    "\n",
    "print(\"\\nMax tokens = 80\")\n",
    "print(gen(prompt, max_new_tokens=80)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f09a01-8a3c-479a-a54c-c5584a88926e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc17859-29fa-4543-bdea-0b49136acfab",
   "metadata": {},
   "source": [
    "### 2) `do_sample` -> deterministic vs varied outputs\n",
    "\n",
    "By default, most pipelines behave like:\n",
    "- `do_sample=False` ‚Üí **deterministic** (same prompt ‚Üí same output)\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "gen(prompt, do_sample=False)\n",
    "gen(prompt, do_sample=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c35c0-0e8c-4d12-880c-b92d36356172",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257a8f3-20ad-4add-992a-856e4fd79bfc",
   "metadata": {},
   "source": [
    "When a model generates text, it chooses words one by one.\n",
    "Each next word has probabilities ‚Äî like:\n",
    "\n",
    "Token | Probability\n",
    "------|-------------\n",
    "dog   | 0.65\n",
    "cat   | 0.25\n",
    "wolf  | 0.10\n",
    "\n",
    "`do_sample` tells the model *how* to pick from these probabilities.\n",
    "\n",
    "-------------------------------------------------------------\n",
    "üß© do_sample = False   ‚Üí  DETERMINISTIC MODE\n",
    "\n",
    "-------------------------------------------------------------\n",
    "- Always picks the word with the highest score.\n",
    "- No randomness.\n",
    "- Same prompt ‚Üí same output every time.\n",
    "- ‚úÖ Called ‚Äúdeterministic‚Äù because the result is fixed and repeatable.\n",
    "\n",
    "-------------------------------------------------------------\n",
    "üé≤ do_sample = True   ‚Üí  STOCHASTIC MODE\n",
    "\n",
    "-------------------------------------------------------------\n",
    "- Randomly picks from the probability list.\n",
    "- Sometimes ‚Äúcat,‚Äù sometimes ‚Äúdog,‚Äù depending on luck and temperature.\n",
    "- Same prompt ‚Üí slightly different outputs each time.\n",
    "- üé® Called ‚Äústochastic‚Äù because randomness is part of the process.\n",
    "\n",
    "-------------------------------------------------------------\n",
    "\n",
    "When sampling is ON (`do_sample=True`), we can control *how* it samples using `temperature` and `top_p`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22c73bf-24e2-4976-b0a8-6150bfe6333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do_sample = False\n",
      "The dog is a dog. He is a dog. He is a hero. He is a dog. He is a hero.\n"
     ]
    }
   ],
   "source": [
    "# do_sample = False\n",
    "\n",
    "print(\"do_sample = False\")\n",
    "print(gen(prompt, do_sample=False, max_new_tokens=40)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a0543f-9ab9-450a-8b57-25432f83bff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "do_sample = True\n",
      "The Little Puppy is a German Shepherd dog with a nose. The owner teaches him how to recognize and care for the dog by examining the thigh. Soon the puppy begins\n"
     ]
    }
   ],
   "source": [
    "# do_sample = True\n",
    "\n",
    "print(\"\\ndo_sample = True\")\n",
    "print(gen(prompt, do_sample=True, max_new_tokens=40)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a578e54-8359-49b6-b5c5-87aeee0aec17",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f46a3-85e6-45de-ac48-6c3a16f06a19",
   "metadata": {},
   "source": [
    "### 3) `temperature` (randomness / creativity)\n",
    "\n",
    "**Important:** `temperature` works only when `do_sample=True`.\n",
    "\n",
    "**How it works (intuition):**\n",
    "- At each step, the model has multiple possible next tokens with probabilities.\n",
    "- `temperature` reshapes those probabilities:\n",
    "  - Lower temperature (e.g., 0.2) ‚Üí makes the top choices **more dominant**\n",
    "    ‚Üí safer/more predictable wording\n",
    "  - Higher temperature (e.g., 1.2+) ‚Üí flattens probabilities\n",
    "    ‚Üí more variety, but higher risk of odd/off-topic outputs\n",
    "\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "gen(prompt, do_sample=True, temperatur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf543fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature = 0.2\n",
      "The dog is a dog. He is a dog. He is a hero. He is a hero.\n"
     ]
    }
   ],
   "source": [
    "# TEMPERATURE -> controls creativity / randomness\n",
    "# Lower = predictable, Higher = more creative/varied\n",
    "\n",
    "print(\"Temperature = 0.2\")\n",
    "print(gen(prompt, temperature=0.2, do_sample=True, max_new_tokens=60)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ae6059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature = 1.5\n",
      "A small black dog runs inside under a bed of blue smoke drifting away out of air. A little green light comes over the head of the dog, pointing for where the fire belongs, hoping that someone will rescue it; but quickly she stumbles on a cat's head and\n"
     ]
    }
   ],
   "source": [
    "# TEMPERATURE -> controls creativity / randomness\n",
    "# Lower = predictable, Higher = more creative/varied\n",
    "\n",
    "print(\"\\nTemperature = 1.5\")\n",
    "print(gen(prompt, temperature=1.5, do_sample=True, max_new_tokens=60)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5e60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature = 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog is a dog. He is a dog. He is a hero. He is a dog. He is a hero.\n"
     ]
    }
   ],
   "source": [
    "# TEMPERATURE -> controls creativity / randomness\n",
    "# Lower = predictable, Higher = more creative/varied\n",
    "\n",
    "print(\"\\nTemperature = 1.5\")\n",
    "print(gen(prompt, temperature=1.5, do_sample=False, max_new_tokens=60)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62998a",
   "metadata": {},
   "source": [
    "**What have we learnt?**\n",
    "\n",
    "Temperature doesn‚Äôt make the model ‚Äúsmarter‚Äù ‚Äî it changes how risky it is when picking the next word.\n",
    "\n",
    "- Low temp: always picks the safest word ‚Üí same output every time.\n",
    "- High temp: takes more risks ‚Üí new ideas, but more errors.\n",
    "- Works only if sampling is on (do_sample=True)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce7f92-b142-45cd-9957-e31f39c00ccc",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5b0bc-bacd-4685-9a14-823251d783f8",
   "metadata": {},
   "source": [
    "### 4) `top_p` (nucleus sampling: how wide the choice pool is)\r\n",
    "\r\n",
    "**Important:** `top_p` is used when `do_sample=True`.\r\n",
    "\r\n",
    "**How it works (intuition):**\r\n",
    "Instead of considering every possible next token, `top_p` picks a *set* of tokens whose\r\n",
    "probabilities add up to `p`, then samples from that set.\r\n",
    "\r\n",
    "- `top_p=0.1` ‚Üí very narrow pool (only the most likely tokens)\r\n",
    "  ‚Üí more focused / less diverse wording\r\n",
    "- `top_p=0.9` ‚Üí wider pool\r\n",
    "  ‚Üí more diverse wording\r\n",
    "\r\n",
    "**Syntax:**\r\n",
    "```python\r\n",
    "gen(prompt, do_sample=True, top_p=0.9)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3936b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p = 0.3\n",
      "The dog is a dog. He is a dog. He is a hero. He is a dog. He is a hero.\n"
     ]
    }
   ],
   "source": [
    "# TOP-P -> controls diversity of word choices\n",
    "# Lower = focused on the most likely words\n",
    "# Higher = allows more diverse / unexpected words\n",
    "\n",
    "print(\"Top-p = 0.3\")\n",
    "print(gen(prompt, top_p=0.1, do_sample=True, max_new_tokens=60)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3b70200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p = 0.9\n",
      "The dog who was injured a few years ago lost his owner. Seeing the dog's owner in the street, he began to take a liking to him. His owner became more than a dog.\n"
     ]
    }
   ],
   "source": [
    "# TOP-P -> controls diversity of word choices\n",
    "# Lower = focused on the most likely words\n",
    "# Higher = allows more diverse / unexpected words\n",
    "\n",
    "print(\"Top-p = 0.9\")\n",
    "print(gen(prompt, top_p=0.9, do_sample=True, max_new_tokens=60)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b57a7b4",
   "metadata": {},
   "source": [
    "### Reflection\r\n",
    "\r\n",
    "1) If you want the **same output every time** for the same prompt, which setting matters most?\r\n",
    "   - **Hint:** Look at `do_sample`.\r\n",
    "\r\n",
    "2) If your responses are **too repetitive**, what would you change first?\r\n",
    "   - **Hint:** Turn ON sampling, then adjust randomness.\r\n",
    "\r\n",
    "3) If outputs become **too weird/off-topic**, what two parameters could you lower to make it safer?\r\n",
    "   - **Hint:** Both are used when sampling is ON.\r\n",
    "\r\n",
    "4) Suppose you need a **very short answer** (1 sentence). Which parameter directly controls that?\r\n",
    "   - **Hint:** This parameter limits length.\r\n",
    "\r\n",
    "5) Two students used the same prompt and got different outputs. What is the most likely reason?\r\n",
    "   - **Hint:** Sampling changes whether outputs can vary.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034208e-af51-46d0-882f-f50192ff5919",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f101b-97f0-4865-a42f-c6a880beead5",
   "metadata": {},
   "source": [
    "## Full demo: two classroom-safe use cases (tuned differently)\r\n",
    "\r\n",
    "We will use the **same parameters** but choose different values depending on the goal:\r\n",
    "- Use Case A: ‚ÄúReliable and safe‚Äù (more deterministic / focused)\r\n",
    "- Use Case B: ‚ÄúMore creative‚Äù (more variety, but still classroom-safe)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a93e3e-3cc2-431b-a6d4-11e670ae7236",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc40d964-8dc6-44ee-83af-e70e942dc7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Case A (Reliable/Safe)\n",
      "Task: Summarize machine learning in one line:\n",
      "Output:\n",
      " What are the benefits of machine learning?\n"
     ]
    }
   ],
   "source": [
    "task_a = \"Summarize machine learning in one line:\"\n",
    "prompt_a = safe_prompt(task_a)\n",
    "\n",
    "response_a = gen(\n",
    "    prompt_a,\n",
    "    max_new_tokens=120,   # enough space for bullets\n",
    "    do_sample=True,      # allow some flexibility in phrasing\n",
    "    temperature=0.3,     # low randomness (more reliable)\n",
    "    top_p=0.3            # narrow choice pool (more focused)\n",
    ")\n",
    "\n",
    "print(\"Use Case A (Reliable/Safe)\")\n",
    "print(\"Task:\", task_a)\n",
    "print(\"Output:\\n\", response_a[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ceab-1a1c-44ea-a6b8-d8dbd8ff7266",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7429c1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use Case B (Creative but Classroom-Safe)\n",
      "Task: Write a short, encouraging story (4 sentences) about teamwork during a college project.\n",
      "Output:\n",
      " All of the students had a very successful project. My wife, who was a biology major, gave us a lot of help. I really enjoyed the project. But as the project progressed, the team struggled to stay on top. The teacher, who was a freshman, recommended that we use the project as a team and work together. My wife thought it was a great project. My wife, who was a freshman, was a little nervous about it. She also wanted to help out.\n"
     ]
    }
   ],
   "source": [
    "task_b = \"Write a short, encouraging story (4 sentences) about teamwork during a college project.\"\n",
    "prompt_b = safe_prompt(task_b)\n",
    "\n",
    "response_b = gen(\n",
    "    prompt_b,\n",
    "    max_new_tokens=180,  # more room for storytelling\n",
    "    do_sample=True,      # variation required for creativity\n",
    "    temperature=0.9,     # higher randomness (more creative)\n",
    "    top_p=0.9            # wider pool (more diverse word choice)\n",
    ")\n",
    "\n",
    "print(\"\\nUse Case B (Creative but Classroom-Safe)\")\n",
    "print(\"Task:\", task_b)\n",
    "print(\"Output:\\n\", response_b[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5965e-156e-455c-bf15-e49bc01e2377",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9478dc8-0002-4add-93ca-5e483ca94125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
