{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b26590",
   "metadata": {},
   "source": [
    "# üß† In-Class Exercise: Building Your First LLM Chatbot\n",
    "\n",
    "Welcome!  \n",
    "This notebook is your hands-on lab for **Session 3 ‚Äì Building and Tuning LLMs**.  \n",
    "You‚Äôll go step-by-step through concepts we discussed in class ‚Äî pipelines, parameters, and model behavior ‚Äî and try small experiments to understand how LLMs actually ‚Äúthink.‚Äù  \n",
    "\n",
    "#### What you will practice\r\n",
    "1) Choosing the right model for the task  \r\n",
    "2) Controlling outputs using generation parameters  \r\n",
    "3) Creating a simple Streamlit LLM app\n",
    "\n",
    "Let‚Äôs get started üöÄ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96395b7-04dc-493a-b7e7-0a3a42712880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Classroom-safe prompt wrapper (use for all tasks)\n",
    "def safe_prompt(task: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful assistant for students. \"\n",
    "        \"Keep responses polite, non-explicit, and suitable for a classroom.\\n\"\n",
    "        f\"Task: {task}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad1381",
   "metadata": {},
   "source": [
    "## üß© Concept 1: The Hugging Face Pipeline\n",
    "\n",
    "**Theory Recap:**  \n",
    "A pipeline is like a ‚Äúready-made tool‚Äù that connects your text input to an AI model.  \n",
    "Instead of manually loading weights and tokenizers, we use a *pipeline* for common tasks such as summarization, translation, and text generation.\n",
    "\n",
    "Different models are trained for different purposes:\n",
    "- `flan-t5-small` ‚Üí instruction-following / Q&A  \n",
    "- `distilgpt2` ‚Üí text continuation  \n",
    "- `microsoft/DialoGPT-small` ‚Üí dialogue/chat  \n",
    "\n",
    "Each model has its own strengths.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6261d628",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Use artificial intelligence to help automate tasks.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example: Create a simple pipeline and use it\n",
    "\n",
    "# Step 1: Choose your task and model\n",
    "task = \"text2text-generation\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Step 2: Create the pipeline\n",
    "gen = pipeline(task, model=model_name)\n",
    "\n",
    "# Step 3: Try it out\n",
    "prompt = \"Summarize: Artificial intelligence helps automate tasks.\"\n",
    "response = gen(prompt, max_new_tokens=60)\n",
    "print(\"Output:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0e016",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† TASK 1 (Guided Practice)\n",
    "# Use a different model - distilgpt2\n",
    "# 1. Change the task to \"text-generation\"\n",
    "# 2. Use model_name = \"distilgpt2\"\n",
    "# 3. Create your own prompt like \"Once upon a time...\"\n",
    "\n",
    "# Your code below üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af04f312-d59a-4014-b48d-04d1f21d7921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAN says:\n",
      "Make sure you have a good first impression.\n",
      "\n",
      "distilGPT2 says:\n",
      "How do I make a good first impression? Or perhaps someone else could provide some guidance as to the concept.\n",
      "\n",
      "\n",
      "The first part of the script is that the script is going to look like this:\n",
      "What the hell is going to happen to the man who wrote it?\n",
      "It has been announced that if the men in the film\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example: Compare FLAN vs DialoGPT on the same input\n",
    "\n",
    "# Model 1: Instruction-following model\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "# Model 2: Autocomplete-style text generation model\n",
    "distilgpt = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Same intent, different prompt styles\n",
    "prompt_flan = \"Give 3 tips for making a good first impression:\"\n",
    "prompt_gpt  = \"How do I make a good first impression?\"\n",
    "\n",
    "response_flan = flan(prompt_flan, max_new_tokens=60)\n",
    "response_gpt  = distilgpt(prompt_gpt, max_new_tokens=60)\n",
    "\n",
    "print(\"FLAN says:\")\n",
    "print(response_flan[0][\"generated_text\"])\n",
    "\n",
    "print(\"\\ndistilGPT2 says:\")\n",
    "print(response_gpt[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f54210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° Task 2 (Critical Thinking)\n",
    "# What happens if you swap the prompts for the two models?\n",
    "# If you want **bullet points**, which model + prompt style is better?\n",
    "# Try using \"text2text-generation\" instead of \"text-generation\" for distilgpt2. What happens?\n",
    "# Write your observation in a comment below & discuss with your groupüëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8efdf",
   "metadata": {},
   "source": [
    "## üß© Concept 2: Controlling Model Creativity\n",
    "\n",
    "**Theory Recap:**  \n",
    "Parameters like `temperature`, `top_p`, and `max_new_tokens` control how ‚Äúcreative‚Äù or ‚Äúfocused‚Äù the model‚Äôs output is.  \n",
    "- **Temperature**: randomness (0 = deterministic, 1 = more creative).  \n",
    "- **Top-p**: diversity of words considered.  \n",
    "- **Max new tokens**: how long the response can be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44762226-bce7-48ea-86ac-d96b782612ef",
   "metadata": {},
   "source": [
    "#### A) `max_new_tokens` (controls response length)\r\n",
    "\r\n",
    "Syntax:\r\n",
    "```python\r\n",
    "response = gen_base(prompt, max_new_tokens=60)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f24c544c-ff8d-4069-9dd0-b583d08f1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_new_tokens = 20\n",
      "Cloud computing is a type of computing that uses a lot of resources, including computer resources,\n",
      "\n",
      "max_new_tokens = 120\n",
      "Cloud computing is a type of computing that uses a lot of resources, including computer resources, to store and store data.\n"
     ]
    }
   ],
   "source": [
    "# Code example\n",
    "\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "prompt = \"Explain cloud computing in simple terms (student-friendly):\"\n",
    "\n",
    "response_short = flan(prompt, max_new_tokens=20)\n",
    "response_long = flan(prompt, max_new_tokens=120)\n",
    "# prompt = safe_prompt(task)\n",
    "\n",
    "print(\"max_new_tokens = 20\")\n",
    "print(response_short[0][\"generated_text\"])\n",
    "\n",
    "print(\"\\nmax_new_tokens = 120\")\n",
    "print(response_long[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9954ff8-2cbf-43b4-b7c5-7cb001ef2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1 (max_new_tokens)\n",
    "# Play with 'max_new_tokens'\n",
    "# 1. Generate a short version (20 tokens)\n",
    "# 2. Generate a longer version (80 tokens)\n",
    "# Observe the difference in length and tone.\n",
    "\n",
    "prompt = \"Describe a sunset.\"\n",
    "# Your code below üëá\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d85040-7a13-4e78-b61f-67f2b75a3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2 (max_new_tokens)\n",
    "# You want a **short title generator**.\n",
    "# Update the prompt + `max_new_tokens` so the output is:\n",
    "# - **one short title only**\n",
    "\n",
    "# Hint: ask for ‚Äúexactly 1 title‚Äù + keep tokens low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bade44-2e9b-420e-b3e8-fa29e33d72e9",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633734a-a49c-42bb-9f49-83a41d61ed68",
   "metadata": {},
   "source": [
    "#### B) `do_sample` (controls variation)\r\n",
    "\r\n",
    "- `do_sample=False` ‚Üí more deterministic\r\n",
    "- `do_sample=True`  ‚Üí more varied\r\n",
    "\r\n",
    "Syntax:\r\n",
    "```python\r\n",
    "flan(prompt, do_sample=True)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3603446-ee18-4f8f-8ad9-fea4ab63341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do_sample = False\n",
      "Using a calculator, you can find the number of lines in a line.\n",
      "\n",
      "do_sample = True\n",
      "The best way of practicing a concept is to use a mixture of several notes. Practice is key; and the best way to learn it is to study it often\n"
     ]
    }
   ],
   "source": [
    "# Code Example\n",
    "\n",
    "flan_base = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "prompt = \"Give 1 study tip for learning Python:\"\n",
    "\n",
    "response_false = flan_base(prompt, do_sample = False, max_new_tokens=60)\n",
    "response_true = flan_base(prompt, do_sample = True, max_new_tokens=60)\n",
    "# prompt = safe_prompt(task)\n",
    "\n",
    "print(\"do_sample = False\")\n",
    "print(response_false[0][\"generated_text\"])\n",
    "\n",
    "print(\"\\ndo_sample = True\")\n",
    "print(response_true[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399f87-9602-4257-b0de-4981bbf11cb7",
   "metadata": {},
   "source": [
    "##### Task 1 (do_sample)\n",
    "Run `do_sample=True` **3 times**. Did you get different outputs?\n",
    "\n",
    "Then run `do_sample=False` **3 times**. Did it stay similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb47917e-6607-4fb9-8bbd-ce66cde36534",
   "metadata": {},
   "source": [
    "##### üí° Task 2 (Critical Thinking)\n",
    "You are building a **FAQ bot** for students where answers must be consistent.\n",
    "Which is better: `do_sample=True` or `do_sample=False`? \n",
    "\n",
    "Discuss with your group. What other cases can you think of when do_sample is true or when do_sample is false?\n",
    "Hint: consistency ‚Üí deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132063fd-6981-4b1c-9f3e-7d93aa0beaff",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a66af7-b764-425e-ae50-8f9fdbb7c845",
   "metadata": {},
   "source": [
    "#### C) `temperature` (randomness when sampling)\n",
    "\n",
    "Works best when `do_sample=True`.\n",
    "\n",
    "- low (0.2) ‚Üí safer / more predictable\n",
    "- higher (1.2) ‚Üí more variety / higher risk\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "gen(prompt, do_sample=True, temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b4af881",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low temperature: The teamwork is a key part of the team's success.\n",
      "High temperature: The company was going to get their approval for the upcoming campaign in March 2017, this time with a larger group than expected, and we saw \n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example: Comparing low vs high temperature\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "prompt = \"Write a one-line quote about teamwork.\"\n",
    "\n",
    "response_low = generator(prompt, do_sample = True, temperature=0.2, max_new_tokens=30)\n",
    "response_high = generator(prompt, do_sample = True, temperature=1.2, max_new_tokens=30)\n",
    "\n",
    "print(\"Low temperature:\", response_low[0][\"generated_text\"])\n",
    "print(\"High temperature:\", response_high[0][\"generated_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f53fbed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† Task 1 (Guided Practice)\n",
    "# Play with 'temperature'\n",
    "# 1. Generate responses with temperature values equal to 0.2, 0.7 and 1.2\n",
    "# Observe the difference, discuss in your group how it differs.\n",
    "\n",
    "prompt = \"Describe a sunset.\"\n",
    "# Your code below üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b09ff8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° Task 2 (Critical Thinking)\n",
    "# You want a **creative tagline generator** for a college event poster.\n",
    "# Should temperature go up or down? Why?\n",
    "# Code your creative tagline generator and discuss with the group.\n",
    "# Hint: creativity usually increases when temperature increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1479e-6cf9-48fd-8826-b18ec6ea7c69",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8d8d8-a3a7-4260-929d-b63594a6d932",
   "metadata": {},
   "source": [
    "#### D) `top_p` (nucleus sampling: size of choice pool)\n",
    "\n",
    "Works best when `do_sample=True`.\n",
    "\n",
    "- `top_p=0.1` ‚Üí narrow choices (more focused)\n",
    "- `top_p=0.9` ‚Üí wider choices (more diverse)\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "gen(prompt, do_sample=True, top_p=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f39f453-832b-45ce-bc00-2ca9d7555146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p = 0.1\n",
      "Teamwork is the key to success.\n",
      "\n",
      "top_p = 0.9\n",
      "Teamwork is a key component of a successful business.\n"
     ]
    }
   ],
   "source": [
    "# Code Example\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "prompt = \"Give exactly 3 bullet points on why teamwork matters:\"\n",
    "\n",
    "response_low_p = generator(prompt, do_sample=True, temperature=0.3, top_p=0.1, max_new_tokens=80)\n",
    "response_high_p = generator(prompt, do_sample=True, temperature=0.3, top_p=0.9, max_new_tokens=80)\n",
    "\n",
    "print(\"top_p = 0.1\")\n",
    "print(response_low_p[0][\"generated_text\"])\n",
    "\n",
    "print(\"\\ntop_p = 0.9\")\n",
    "print(response_high_p[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f9412-a2d6-480e-948d-4b3f4537aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Task 1 (Guided Practice)\n",
    "# Play with 'top_p'\n",
    "# 1. Generate responses with top_p values equal to 0.2, 0.5 and 0.9\n",
    "# Which gives the best balance of clarity + variety? Discuss in your group.\n",
    "\n",
    "prompt = \"Enter your prompt here\"\n",
    "# Your code below üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9430f-5d8d-4bd4-bac9-5fb1efab269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí° Task 2 (Critical Thinking)\n",
    "# You are generating **formal email replies** for students.\n",
    "# Should `top_p` be lower or higher? Why?\n",
    "# Hint: formal + consistent ‚Üí narrower pool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e2933-bfc1-42fa-9a56-14920b5bf122",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c91d35-967d-49aa-95a0-0c10182ae8dd",
   "metadata": {},
   "source": [
    "### Use all parameters together (applied)\n",
    "\n",
    "Use case: ‚ÄúStudy helper‚Äù\n",
    "Output should be:\n",
    "- polite\n",
    "- exactly 3 bullet points\n",
    "- not too long\n",
    "- not too random\n",
    "\n",
    "Tune:\n",
    "- `max_new_tokens`\n",
    "- `do_sample`\n",
    "- `temperature`\n",
    "- `top_p`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b8de177-234d-46f3-a631-ad3eb38fd3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exams are a huge part of our lives, so it is important to make sure you are prepared for them.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Give exactly 3 bullet points on how to prepare for exams effectively:\"\n",
    ")\n",
    "# prompt = safe_prompt(task)\n",
    "\n",
    "response = flan(\n",
    "    prompt,\n",
    "    max_new_tokens=60,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.7\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44051606-5d0c-4931-bdf7-8065cbfb892e",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf2c72",
   "metadata": {},
   "source": [
    "## üß© Concept 3: Build a simple Streamlit App\n",
    "\n",
    "**Theory Recap:**  \n",
    "Streamlit helps us build simple web UIs for our chatbot ‚Äî  \n",
    "students can type questions and see AI responses in real time.  \n",
    "\n",
    "We won‚Äôt build the full app here ‚Äî but let‚Äôs preview how the logic works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a77e7-dcf3-413e-84e2-0747fb7dcb94",
   "metadata": {},
   "source": [
    "- **Step 1**: Create a new file: `student_app.py`\n",
    "\n",
    "- **Step 2**: Copy the code from the next cell into that file.\n",
    "\n",
    "- **Step 3 (run in terminal)**: \n",
    "```bash\n",
    "streamlit run student_app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1412223-cd62-4626-91df-ea33caaadcf0",
   "metadata": {},
   "source": [
    "If your file is inside a folder (example: Demo/):\n",
    "```bash\n",
    "streamlit run Demo/student_app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2395b0-0636-4533-aabc-567d8267034a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### ‚úÖ Example: Basic Streamlit chatbot (run later as .py)\n",
    "\n",
    "##### Save this as llm_chatbot_app.py and run: streamlit run llm_chatbot_app.py\n",
    "\n",
    "##### Template students copy into .py\n",
    "```python\n",
    "# Copy this into: student_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "st.set_page_config(page_title=\"Student LLM App\", page_icon=\"üß†\")\n",
    "st.title(\"üß† Student LLM App\")\n",
    "\n",
    "gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "def safe_prompt(task: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful assistant for students. \"\n",
    "        \"Keep responses polite, non-explicit, and suitable for a classroom.\\n\"\n",
    "        f\"Task: {task}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "max_new_tokens = st.slider(\"Max new tokens\", 16, 200, 80, 8)\n",
    "user_text = st.text_area(\"Enter a classroom-safe question/task:\")\n",
    "\n",
    "if st.button(\"Generate\"):\n",
    "    if not user_text.strip():\n",
    "        st.warning(\"Please type something first.\")\n",
    "    else:\n",
    "        prompt = safe_prompt(user_text)\n",
    "        output = gen(prompt, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
    "        st.subheader(\"LLM Output\")\n",
    "        st.write(output)\n",
    "\n",
    "# Easy edits:\n",
    "# 1) Switch model to google/flan-t5-base\n",
    "# 2) Change the prompt to request bullets / steps / 1 sentence\n",
    "# 3) Change max_new_tokens default\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d8755",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† Task 1 (Homework Practice)\n",
    "# 1. In your Streamlit app file, add a sidebar slider for 'max_new_tokens'.\n",
    "# 2. Let the user control the answer length interactively.\n",
    "# 3. Test how the response changes for small vs large values.\n",
    "# (You don‚Äôt have to run Streamlit here, just plan the code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111f9b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üí° Task 2 (Critical Thinking)\n",
    "# Think about a new feature you‚Äôd add if you had more time.\n",
    "# Example ideas:\n",
    "# - A dropdown to choose between models\n",
    "# - A toggle for ‚Äúcreative‚Äù vs ‚Äúprecise‚Äù mode\n",
    "# - Saving previous chat responses\n",
    "# Write your idea below üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58647c92",
   "metadata": {},
   "source": [
    "## üß≠ Wrap-Up & Look-Ahead Reflection\n",
    "\n",
    "### üéì What You Learned Today\n",
    "- How to use the **Hugging Face pipeline** to connect prompts ‚Üí models  \n",
    "- How **parameters** like temperature, top-p, and tokens change model behavior  \n",
    "- How to pick the **right model** for a given task (Flan vs GPT vs DialoGPT)  \n",
    "- How a simple **Streamlit UI** turns code into an interactive chatbot  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11c63e-99df-4e46-985f-3bc6a5585bc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üéØ **Challenge for the Curious:**  \n",
    "Write down one ‚Äúpain point‚Äù you noticed while testing your chatbot today.  \n",
    "What felt limited or frustrating ‚Äî and what would you love to improve if you could?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9579ad-2d42-4247-b4d5-628223b604ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0fc79b0-38b0-40c9-a4c1-c2c532de449b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üí¨ Think About‚Ä¶\n",
    "1. Our chatbot only knows what‚Äôs inside its model ‚Äî it can‚Äôt answer about *your* documents or notes.  \n",
    "   - How could we make it read PDFs or data files and respond using that knowledge?  \n",
    "\n",
    "2. Today‚Äôs bot handles one message at a time.  \n",
    "   - What if you wanted several ‚Äúmini-bots‚Äù ‚Äî one to search, one to plan, one to answer ‚Äî all working together?  \n",
    "\n",
    "3. Our model always starts fresh ‚Äî it forgets previous questions.  \n",
    "   - How could a chatbot remember your last conversation or build on context?  \n",
    "\n",
    "4. Curious minds only üöÄ  \n",
    "   - Ever wondered how these models can be **fine-tuned** on your own data, or how voice assistants use them in real time?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc974f2-33c8-49a1-8026-cb397a14e8ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
